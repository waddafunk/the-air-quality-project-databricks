{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bd20214-f29c-45b5-8d09-ad1bcb8cd61b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, month, to_timestamp\n",
    "import datetime\n",
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, inspect\n",
    "from pyspark.sql.window import Window\n",
    "import psycopg2\n",
    "from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2c4720c-77ce-494e-9747-3f115269ea84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "POSTGRES_USER = os.environ[\"POSTGRES_USER\"]\n",
    "POSTGRES_PASSWORD = os.environ[\"POSTGRES_PASSWORD\"]\n",
    "POSTGRES_HOST = os.environ[\"POSTGRES_HOST\"]\n",
    "POSTGRES_PORT = os.environ[\"POSTGRES_PORT\"]\n",
    "OPENAQ_BUCKET = \"openaq-data-archive\"\n",
    "OPENAQ_BUCKET_URI = f\"s3://{OPENAQ_BUCKET}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ec10daa-9257-415f-9522-9a2350e5e2a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_feature_store_if_not_exists():\n",
    "    \"\"\"Create the 'feature-store' database if it doesn't exist\"\"\"\n",
    "    # Get PostgreSQL connection details from environment variables\n",
    "    pg_host = os.environ.get('POSTGRES_HOST')\n",
    "    pg_user = os.environ.get('POSTGRES_USER')\n",
    "    pg_password = os.environ.get('POSTGRES_PASSWORD')\n",
    "    \n",
    "    # Connect to the default 'postgres' database first\n",
    "    conn = psycopg2.connect(\n",
    "        host=pg_host,\n",
    "        user=pg_user,\n",
    "        password=pg_password,\n",
    "        dbname='postgres',\n",
    "        port=os.environ.get(\"POSTGRES_PORT\")\n",
    "    )\n",
    "    \n",
    "    # We need to set autocommit to create a database\n",
    "    conn.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Check if the database exists\n",
    "    cursor.execute(\"SELECT 1 FROM pg_database WHERE datname='feature-store'\")\n",
    "    exists = cursor.fetchone()\n",
    "    \n",
    "    if not exists:\n",
    "        cursor.execute(\"CREATE DATABASE \\\"feature-store\\\"\")\n",
    "        print(\"Database 'feature-store' created successfully\")\n",
    "    else:\n",
    "        print(\"Database 'feature-store' already exists\")\n",
    "    \n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "create_feature_store_if_not_exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7b2a802-469f-440c-96a4-b5fc842ffedd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "connection_string = f\"postgresql://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_HOST}:{POSTGRES_PORT}/openaq\"\n",
    "\n",
    "# Create the connection engine\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Fetch tables using pandas\n",
    "countries = pd.read_sql_table(\"countries\", engine)\n",
    "locations = pd.read_sql_table(\"locations\", engine)\n",
    "\n",
    "\n",
    "# Close the connection\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91bc19b3-16da-41c4-9c1a-8aae24520088",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "countries[\"name\"] = countries[\"name\"].replace({\"Russian Federation\": \"Russia\"})\n",
    "countries[\"name\"].sort_values().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0571435c-0fdd-4bec-bbde-25bf07122a09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "connection_string = f\"postgresql://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_HOST}:{POSTGRES_PORT}/energy\"\n",
    "\n",
    "# Create the connection engine\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Create an inspector object\n",
    "inspector = inspect(engine)\n",
    "\n",
    "# Get all table names\n",
    "db_table_names = inspector.get_table_names()\n",
    "\n",
    "\n",
    "# Close the connection\n",
    "engine.dispose()\n",
    "\n",
    "db_table_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5b14069-b71f-4f08-ad00-da619a990e24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_names = pd.Series(db_table_names).map(lambda x: x.replace(\"energy_\", \"\"))\n",
    "matched_filter = table_names.map(lambda x: x.lower().replace(\"_\", \" \")).isin(\n",
    "    countries[\"name\"].map(lambda x: x.lower().replace(\"_\", \" \"))\n",
    ")\n",
    "matched_filter.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bac3efb4-df29-4b80-abdb-2ea429728915",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_names = table_names[matched_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e55a9ee2-5635-498a-9ead-a3431a92b32a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def check_and_recreate_table(engine, table_name, df=None):\n",
    "    inspector = inspect(engine)\n",
    "    table_exists = table_name in inspector.get_table_names()\n",
    "    \n",
    "    if table_exists:\n",
    "        print(f\"Table '{table_name}' exists. Dropping it.\")\n",
    "        engine.execute(f\"DROP TABLE {table_name}\")\n",
    "        print(f\"Table '{table_name}' dropped.\")\n",
    "    \n",
    "    # Create the table\n",
    "    if df is not None:\n",
    "        # Create from DataFrame\n",
    "        print(f\"Creating table '{table_name}' from DataFrame.\")\n",
    "        df.to_sql(table_name, engine, index=False, if_exists='replace')\n",
    "        print(f\"Table '{table_name}' created and data uploaded.\")\n",
    "    else:\n",
    "        raise ValueError(\"Either df or table_definition must be provided to create the table\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Improved function for more efficient reading using partition pruning\n",
    "def read_openaq_with_partition_pruning(location_ids, start_year=2024):\n",
    "    \"\"\"\n",
    "    More efficient version using Spark's built-in partition pruning\n",
    "\n",
    "    Args:\n",
    "        location_ids (list): List of location IDs to include\n",
    "        start_year (int): Minimum year to include (default: 2024)\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Filtered OpenAQ data\n",
    "    \"\"\"\n",
    "    # Convert location IDs to strings\n",
    "    location_ids_str = [str(id) for id in location_ids]\n",
    "\n",
    "    # Build location pattern\n",
    "    location_pattern = \"{\" + \",\".join(location_ids_str) + \"}\"\n",
    "\n",
    "    # Build year pattern - get all years from start_year to current\n",
    "    current_year = datetime.datetime.now().year\n",
    "    years = range(start_year, current_year + 1)\n",
    "    year_pattern = \"{\" + \",\".join(str(y) for y in years) + \"}\"\n",
    "\n",
    "    # Construct path with glob patterns\n",
    "    glob_path = f\"{OPENAQ_BUCKET_URI}/records/csv.gz/locationid={location_pattern}/year={year_pattern}/month=*/*.csv.gz\"\n",
    "\n",
    "    # Read with glob pattern - Spark will efficiently prune partitions\n",
    "    print(f\"Reading data with glob pattern: {glob_path}\")\n",
    "\n",
    "    try:\n",
    "        df = (\n",
    "            spark.read.format(\"csv\")\n",
    "            .option(\"header\", \"true\")\n",
    "            .option(\"inferSchema\", \"true\")\n",
    "            .option(\"compression\", \"gzip\")\n",
    "            .load(glob_path)\n",
    "        )\n",
    "\n",
    "        # Convert datetime string to timestamp\n",
    "        df = df.withColumn(\"datetime\", to_timestamp(col(\"datetime\")))\n",
    "\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading data: {e}\")\n",
    "        # Return empty dataframe with expected schema\n",
    "        return spark.createDataFrame(\n",
    "            [],\n",
    "            [\n",
    "                \"location_id\",\n",
    "                \"sensors_id\",\n",
    "                \"location\",\n",
    "                \"datetime\",\n",
    "                \"lat\",\n",
    "                \"lon\",\n",
    "                \"parameter\",\n",
    "                \"units\",\n",
    "                \"value\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "\n",
    "def create_feature_store_country_data_by_sensor(country, openaq_countries, openaq_locations, energy_engine, feature_store_engine):\n",
    "    db_country_name = \"energy_\" + country\n",
    "    country_words = list(map(lambda x: x.capitalize(), country.split(\"_\")))\n",
    "\n",
    "    if len(country_words) > 1:\n",
    "        country = country_words[0].capitalize() + \" \" + country_words[1].capitalize()\n",
    "    else:\n",
    "        country = country_words[0].capitalize()\n",
    "\n",
    "    openaq_country = openaq_countries[openaq_countries[\"name\"].eq(country)]\n",
    "    country_locations = openaq_locations.loc[\n",
    "        openaq_locations[\"country_id\"].isin(\n",
    "            openaq_country[\"id\"]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    energy_data = pd.read_sql_table(db_country_name, engine)\n",
    "    min_year = energy_data[\"year\"].min()\n",
    "\n",
    "    openaq_data = read_openaq_with_partition_pruning(\n",
    "        location_ids=country_locations[\"id\"].to_list(), start_year=min_year\n",
    "    )\n",
    "\n",
    "    share_data = [\n",
    "        col\n",
    "        for col in energy_data.columns\n",
    "        if \"share\" in col\n",
    "        and \"demand\" not in col\n",
    "        and \"energy\" in col\n",
    "        and \"renewables\" not in col\n",
    "        and \"fossil\" not in col\n",
    "    ]\n",
    "\n",
    "    energy_columns = share_data + [\"country\", \"population\", \"gdp\", \"per_capita_electricity\", \"year\"]\n",
    "    energy_data = energy_data[energy_columns]\n",
    "    energy_data = spark.createDataFrame(energy_data)\n",
    "\n",
    "    # First, extract the year as a separate column\n",
    "    openaq_data_with_year = openaq_data.withColumn(\"year\", F.year(\"datetime\"))\n",
    "\n",
    "    # Group by sensor_id and year, getting the last value\n",
    "    # Create a window partitioned by sensor_id and year, ordered by datetime descending\n",
    "    window_spec = Window.partitionBy(\"sensors_id\", \"year\").orderBy(F.desc(\"datetime\"))\n",
    "\n",
    "    # Add row number within each partition\n",
    "    with_row_number = openaq_data_with_year.withColumn(\"row_number\", F.row_number().over(window_spec))\n",
    "\n",
    "    # Filter to keep only the last row for each sensor_id and year combination\n",
    "    yearly_last_values = with_row_number.filter(F.col(\"row_number\") == 1).drop(\"row_number\")\n",
    "\n",
    "    # To get separate tables for each sensor_id, you can use this approach:\n",
    "    # Get unique sensor IDs\n",
    "    sensor_ids = [row.sensors_id for row in openaq_data.select(\"sensors_id\").distinct().collect()]\n",
    "\n",
    "    # Create a dictionary of dataframes, one for each sensor_id\n",
    "    sensor_dataframes = {}\n",
    "    for sensor_id in sensor_ids:\n",
    "        sensor_df = yearly_last_values.filter(F.col(\"sensors_id\") == sensor_id)\n",
    "        sensor_df = sensor_df.join(energy_data, on=\"year\", how=\"inner\")\n",
    "        df_head = sensor_df.select(\"parameter\").head(1)\n",
    "        if len(df_head) != 0:\n",
    "            sensor_dataframes[sensor_id] = sensor_df\n",
    "            parameter = df_head[0][\"parameter\"]\n",
    "\n",
    "            feature_table_name = country.replace(\" \", \"_\") + \"_\" + parameter + \"_sensor_\" + sensor_id\n",
    "\n",
    "            check_and_recreate_table(feature_store_engine, feature_table_name, sensor_df.toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63b424ac-8e11-4073-b787-9236d05e1840",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "connection_string = f\"postgresql://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_HOST}:{POSTGRES_PORT}/energy\"\n",
    "energy_engine = create_engine(connection_string)\n",
    "\n",
    "connection_string = f\"postgresql://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_HOST}:{POSTGRES_PORT}/feature-store\"\n",
    "feature_store_engine = create_engine(connection_string)\n",
    "\n",
    "for country in table_names:\n",
    "    create_feature_store_country_data_by_sensor(country, countries, locations, energy_engine, feature_store_engine)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "eda",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
