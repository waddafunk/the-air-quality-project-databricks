{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bd20214-f29c-45b5-8d09-ad1bcb8cd61b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, month, to_timestamp\n",
    "import datetime\n",
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, inspect\n",
    "import psycopg2\n",
    "from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT\n",
    "from sqlalchemy import inspect\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from pyspark.sql.functions import to_timestamp, col\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2c4720c-77ce-494e-9747-3f115269ea84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "POSTGRES_USER = os.environ[\"POSTGRES_USER\"]\n",
    "POSTGRES_PASSWORD = os.environ[\"POSTGRES_PASSWORD\"]\n",
    "POSTGRES_HOST = os.environ[\"POSTGRES_HOST\"]\n",
    "POSTGRES_PORT = os.environ[\"POSTGRES_PORT\"]\n",
    "OPENAQ_BUCKET = \"openaq-data-archive\"\n",
    "OPENAQ_BUCKET_URI = f\"s3://{OPENAQ_BUCKET}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ec10daa-9257-415f-9522-9a2350e5e2a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_feature_store_if_not_exists():\n",
    "    \"\"\"Create the 'feature-store' database if it doesn't exist\"\"\"\n",
    "    # Get PostgreSQL connection details from environment variables\n",
    "    pg_host = os.environ.get('POSTGRES_HOST')\n",
    "    pg_user = os.environ.get('POSTGRES_USER')\n",
    "    pg_password = os.environ.get('POSTGRES_PASSWORD')\n",
    "    \n",
    "    # Connect to the default 'postgres' database first\n",
    "    conn = psycopg2.connect(\n",
    "        host=pg_host,\n",
    "        user=pg_user,\n",
    "        password=pg_password,\n",
    "        dbname='postgres',\n",
    "        port=os.environ.get(\"POSTGRES_PORT\")\n",
    "    )\n",
    "    \n",
    "    # We need to set autocommit to create a database\n",
    "    conn.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Check if the database exists\n",
    "    cursor.execute(\"SELECT 1 FROM pg_database WHERE datname='feature-store'\")\n",
    "    exists = cursor.fetchone()\n",
    "    \n",
    "    if not exists:\n",
    "        cursor.execute(\"CREATE DATABASE \\\"feature-store\\\"\")\n",
    "        print(\"Database 'feature-store' created successfully\")\n",
    "    else:\n",
    "        print(\"Database 'feature-store' already exists\")\n",
    "    \n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "create_feature_store_if_not_exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7b2a802-469f-440c-96a4-b5fc842ffedd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "connection_string = f\"postgresql://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_HOST}:{POSTGRES_PORT}/openaq\"\n",
    "\n",
    "# Create the connection engine\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Fetch tables using pandas\n",
    "countries = pd.read_sql_table(\"countries\", engine)\n",
    "locations = pd.read_sql_table(\"locations\", engine)\n",
    "\n",
    "\n",
    "# Close the connection\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91bc19b3-16da-41c4-9c1a-8aae24520088",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "countries[\"name\"] = countries[\"name\"].replace({\"Russian Federation\": \"Russia\"})\n",
    "countries[\"name\"].sort_values().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0571435c-0fdd-4bec-bbde-25bf07122a09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "connection_string = f\"postgresql://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_HOST}:{POSTGRES_PORT}/energy\"\n",
    "\n",
    "# Create the connection engine\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Create an inspector object\n",
    "inspector = inspect(engine)\n",
    "\n",
    "# Get all table names\n",
    "db_table_names = inspector.get_table_names()\n",
    "\n",
    "\n",
    "# Close the connection\n",
    "engine.dispose()\n",
    "\n",
    "db_table_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5b14069-b71f-4f08-ad00-da619a990e24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_names = pd.Series(db_table_names).map(lambda x: x.replace(\"energy_\", \"\"))\n",
    "matched_filter = table_names.map(lambda x: x.lower().replace(\"_\", \" \")).isin(\n",
    "    countries[\"name\"].map(lambda x: x.lower().replace(\"_\", \" \"))\n",
    ")\n",
    "matched_filter.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bac3efb4-df29-4b80-abdb-2ea429728915",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_names = table_names[matched_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e55a9ee2-5635-498a-9ead-a3431a92b32a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def check_and_recreate_table(engine, table_name, df=None):\n",
    "    \"\"\"\n",
    "    Check if a table exists and recreate it with new data.\n",
    "    \n",
    "    Args:\n",
    "        engine: SQLAlchemy engine\n",
    "        table_name (str): Name of the table\n",
    "        df (DataFrame): DataFrame containing data to upload\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if successful\n",
    "    \"\"\"\n",
    "    inspector = inspect(engine)\n",
    "    table_exists = table_name in inspector.get_table_names()\n",
    "    \n",
    "    if table_exists:\n",
    "        # Use parameterized query to prevent SQL injection\n",
    "        with engine.begin() as conn:\n",
    "            conn.execute(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "        print(f\"Table '{table_name}' dropped.\")\n",
    "    \n",
    "    # Create the table from DataFrame\n",
    "    if df is not None:\n",
    "        print(f\"Creating table '{table_name}' from DataFrame.\")\n",
    "        df.to_sql(table_name, engine, index=False, if_exists='replace')\n",
    "        print(f\"Table '{table_name}' created and data uploaded.\")\n",
    "    else:\n",
    "        raise ValueError(\"DataFrame must be provided to create the table\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def read_openaq_with_partition_pruning(location_ids, start_year=2024):\n",
    "    \"\"\"\n",
    "    Efficiently read OpenAQ data using Spark's partition pruning.\n",
    "\n",
    "    Args:\n",
    "        location_ids (list): List of location IDs to include\n",
    "        start_year (int): Minimum year to include (default: 2024)\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Filtered OpenAQ data\n",
    "    \"\"\"\n",
    "    # Convert location IDs to strings once\n",
    "    location_ids_str = [str(id) for id in location_ids]\n",
    "\n",
    "    # Early return for empty location list\n",
    "    if not location_ids_str:\n",
    "        return create_empty_openaq_dataframe()\n",
    "\n",
    "    # Build location pattern\n",
    "    location_pattern = \"{\" + \",\".join(location_ids_str) + \"}\"\n",
    "\n",
    "    # Build year pattern - get all years from start_year to current\n",
    "    current_year = datetime.datetime.now().year\n",
    "    year_pattern = \"{\" + \",\".join(str(y) for y in range(start_year, current_year + 1)) + \"}\"\n",
    "\n",
    "    # Construct path with glob patterns\n",
    "    glob_path = f\"{OPENAQ_BUCKET_URI}/records/csv.gz/locationid={location_pattern}/year={year_pattern}/month=*/*.csv.gz\"\n",
    "\n",
    "    print(f\"Reading data with glob pattern: {glob_path}\")\n",
    "\n",
    "    try:\n",
    "        df = (\n",
    "            spark.read.format(\"csv\")\n",
    "            .option(\"header\", \"true\")\n",
    "            .option(\"inferSchema\", \"true\")\n",
    "            .option(\"compression\", \"gzip\")\n",
    "            .load(glob_path)\n",
    "        )\n",
    "\n",
    "        # Convert datetime string to timestamp\n",
    "        return df.withColumn(\"datetime\", to_timestamp(col(\"datetime\")))\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading data: {e}\")\n",
    "        return create_empty_openaq_dataframe()\n",
    "\n",
    "def create_empty_openaq_dataframe():\n",
    "    \"\"\"Create an empty DataFrame with the expected OpenAQ schema.\"\"\"\n",
    "    return spark.createDataFrame(\n",
    "        [],\n",
    "        [\n",
    "            \"location_id\",\n",
    "            \"sensors_id\",\n",
    "            \"location\",\n",
    "            \"datetime\",\n",
    "            \"lat\",\n",
    "            \"lon\",\n",
    "            \"parameter\",\n",
    "            \"units\",\n",
    "            \"value\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "def normalize_country_name(country):\n",
    "    \"\"\"Normalize country name format.\"\"\"\n",
    "    country_words = [word.capitalize() for word in country.split(\"_\")]\n",
    "    return \" \".join(country_words) if len(country_words) > 1 else country_words[0]\n",
    "\n",
    "def get_share_energy_columns(energy_data):\n",
    "    \"\"\"Get relevant energy share columns.\"\"\"\n",
    "    return [\n",
    "        col for col in energy_data.columns\n",
    "        if \"share\" in col\n",
    "        and \"demand\" not in col\n",
    "        and \"energy\" in col\n",
    "        and \"renewables\" not in col\n",
    "        and \"fossil\" not in col\n",
    "    ]\n",
    "\n",
    "def create_feature_store_country_data_by_sensor(country, openaq_countries, openaq_locations, energy_engine, feature_store_engine):\n",
    "    \"\"\"\n",
    "    Create feature store data by sensor for a specific country.\n",
    "    \n",
    "    Args:\n",
    "        country (str): Country name (underscore-separated)\n",
    "        openaq_countries (DataFrame): OpenAQ countries data\n",
    "        openaq_locations (DataFrame): OpenAQ locations data\n",
    "        energy_engine: SQLAlchemy engine for energy data\n",
    "        feature_store_engine: SQLAlchemy engine for feature store\n",
    "    \"\"\"\n",
    "    # Normalize database and display country names\n",
    "    db_country_name = \"energy_\" + country\n",
    "    normalized_country = normalize_country_name(country)\n",
    "    \n",
    "    # Get country and location data\n",
    "    openaq_country = openaq_countries[openaq_countries[\"name\"].eq(normalized_country)]\n",
    "    \n",
    "    # Early return if country not found\n",
    "    if openaq_country.empty:\n",
    "        print(f\"Country '{normalized_country}' not found in OpenAQ data\")\n",
    "        return\n",
    "    \n",
    "    country_locations = openaq_locations.loc[\n",
    "        openaq_locations[\"country_id\"].isin(openaq_country[\"id\"])\n",
    "    ]\n",
    "    \n",
    "    # Early return if no locations found\n",
    "    if country_locations.empty:\n",
    "        print(f\"No locations found for country '{normalized_country}'\")\n",
    "        return\n",
    "\n",
    "    # Read energy data and determine minimum year\n",
    "    try:\n",
    "        energy_data = pd.read_sql_table(db_country_name, energy_engine)\n",
    "        if energy_data.empty:\n",
    "            print(f\"No energy data found for {normalized_country}\")\n",
    "            return\n",
    "            \n",
    "        min_year = energy_data[\"year\"].min()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading energy data: {e}\")\n",
    "        return\n",
    "\n",
    "    # Read OpenAQ data with partition pruning\n",
    "    openaq_data = read_openaq_with_partition_pruning(\n",
    "        location_ids=country_locations[\"id\"].to_list(), \n",
    "        start_year=min_year\n",
    "    )\n",
    "    \n",
    "    # Check if OpenAQ data is empty\n",
    "    if openaq_data.count() == 0:\n",
    "        print(f\"No OpenAQ data found for {normalized_country}\")\n",
    "        return\n",
    "\n",
    "    # Select energy columns and convert to Spark DataFrame\n",
    "    energy_columns = get_share_energy_columns(energy_data) + [\"country\", \"population\", \"gdp\", \"per_capita_electricity\", \"year\"]\n",
    "    energy_data = energy_data[energy_columns]\n",
    "    energy_data = spark.createDataFrame(energy_data)\n",
    "\n",
    "    # Cache frequently used DataFrames\n",
    "    openaq_data.cache()\n",
    "    energy_data.cache()\n",
    "\n",
    "    # Extract year from datetime\n",
    "    openaq_data_with_year = openaq_data.withColumn(\"year\", F.year(\"datetime\"))\n",
    "\n",
    "    # Define window for getting the latest reading per sensor per year\n",
    "    window_spec = Window.partitionBy(\"sensors_id\", \"year\").orderBy(F.desc(\"datetime\"))\n",
    "    \n",
    "    # Add row number and filter to keep only the most recent reading\n",
    "    yearly_last_values = (\n",
    "        openaq_data_with_year\n",
    "        .withColumn(\"row_number\", F.row_number().over(window_spec))\n",
    "        .filter(F.col(\"row_number\") == 1)\n",
    "        .drop(\"row_number\")\n",
    "    )\n",
    "\n",
    "    # Process each sensor\n",
    "    for sensor_id in [row.sensors_id for row in openaq_data.select(\"sensors_id\").distinct().collect()]:\n",
    "        # Filter data for this sensor\n",
    "        sensor_df = yearly_last_values.filter(F.col(\"sensors_id\") == sensor_id)\n",
    "        \n",
    "        # Skip if no data for this sensor\n",
    "        if sensor_df.count() == 0:\n",
    "            continue\n",
    "            \n",
    "        # Get parameter for this sensor\n",
    "        parameter_row = sensor_df.select(\"parameter\").head(1)\n",
    "        if not parameter_row:\n",
    "            continue\n",
    "            \n",
    "        parameter = parameter_row[0][\"parameter\"]\n",
    "        \n",
    "        # Join with energy data\n",
    "        sensor_df = sensor_df.join(energy_data, on=\"year\", how=\"inner\")\n",
    "        \n",
    "        # Skip if join resulted in empty DataFrame\n",
    "        if sensor_df.count() == 0:\n",
    "            continue\n",
    "            \n",
    "        # Create feature table name\n",
    "        feature_table_name = normalized_country.replace(\" \", \"_\") + \"_\" + parameter + \"_sensor_\" + str(sensor_id)\n",
    "        \n",
    "        # Convert to Pandas and save to database\n",
    "        check_and_recreate_table(feature_store_engine, feature_table_name, sensor_df.toPandas())\n",
    "    \n",
    "    # Unpersist cached DataFrames\n",
    "    openaq_data.unpersist()\n",
    "    energy_data.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63b424ac-8e11-4073-b787-9236d05e1840",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "connection_string = f\"postgresql://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_HOST}:{POSTGRES_PORT}/energy\"\n",
    "energy_engine = create_engine(connection_string)\n",
    "\n",
    "connection_string = f\"postgresql://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_HOST}:{POSTGRES_PORT}/feature-store\"\n",
    "feature_store_engine = create_engine(connection_string)\n",
    "\n",
    "for country in table_names:\n",
    "    create_feature_store_country_data_by_sensor(country, countries, locations, energy_engine, feature_store_engine)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "feature extraction",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
